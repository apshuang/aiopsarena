import os
import json
import pytz
import time
import asyncio
import pandas as pd
from pathlib import Path
from zoneinfo import ZoneInfo
from datetime import datetime, timedelta, timezone
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ConnectionTimeout


class TraceExtractor:
    def __init__(self, url: str, username: str, password: str):
        self.elastic = Elasticsearch(
            [url],
            basic_auth=(username, password),
            verify_certs=False,
            request_timeout=60,
            max_retries=5,
            retry_on_timeout=True
        )
        
        # å±è”½ InsecureRequestWarning
        import warnings
        import urllib3
        warnings.filterwarnings("ignore", category=urllib3.exceptions.InsecureRequestWarning)
        self.indices_template = ".ds-traces-apm-default-*"
        
        
    def timezone_adjust(self, local_datetime):
        utc_time = local_datetime.astimezone(pytz.utc)
        # æ ¼å¼åŒ–ä¸º ISO 8601 æ ¼å¼å­—ç¬¦ä¸²
        timestamp_str = utc_time.strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        return timestamp_str
    
    def sort_by_timestamp(self, element):
        return element['timestamp']['us']
    
    def trace_processing(self, traces):
        grouped_trace = {}
        # ä»¥trace idä¸ºkeyï¼Œåˆ›å»ºå­—å…¸ï¼Œå­˜å‚¨data['_source']çš„å†…å®¹
        for trace in traces:
            trace_id = trace['_source']['trace']['id']
            if trace_id in grouped_trace:
                grouped_trace[trace_id].append(trace['_source'])
            else:
                grouped_trace[trace_id] = [trace['_source']]

        timestamp_list = []
        cmdb_id_list = []
        span_id_list = []
        trace_id_list = []
        duration_list = []
        type_list = []
        status_code_list = []
        operation_name_list = []
        parent_span_id = []

        # å°†grouped_traceä¸­æ¯ä¸ªtrace idä¸‹çš„listæŒ‰ç…§æ—¶é—´æˆ³ä»å°åˆ°å¤§æ’åº
        for trace_id, trace_list in grouped_trace.items():
            trace_list = sorted(trace_list, key=self.sort_by_timestamp)
            # ä»æ¯æ¡traceä¸­æå–å‡ºéœ€è¦çš„æ•°æ®
            for trace in trace_list:
                try:
                    # åˆ¤æ–­processorä¸‹çš„nameæ˜¯spanè¿˜æ˜¯transaction
                    processor_name = trace['processor']['event']
                    if 'health' in trace[processor_name]['name'] or 'POST unknown route' in trace[processor_name]['name']:
                        continue
                    span_id_list.append(trace[processor_name]['id'])
                    duration_list.append(trace[processor_name]['duration']['us'])
                    type_list.append(trace[processor_name]['type'])
                    operation_name_list.append(trace[processor_name]['name'])
                    timestamp_list.append(trace['timestamp']['us'])
                    cmdb_id_list.append(trace['service']['name'])
                    trace_id_list.append(trace_id)
                    # åˆ¤æ–­status_codeæ˜¯å¦å­˜åœ¨
                    if 'http' in trace:
                        if 'response' in trace['http']:
                            if 'status_code' in trace['http']['response']:
                                status_code_list.append(trace['http']['response']['status_code'])
                            else:
                                status_code_list.append(0)
                        else:
                            status_code_list.append(0)
                    else:
                        status_code_list.append(0)
                    # åˆ¤æ–­parent_idæ˜¯å¦å­˜åœ¨
                    if 'parent' in trace:
                        parent_span_id.append(trace['parent']['id'])
                    else:
                        parent_span_id.append('')
                except Exception as e:
                    print(trace)

        # åˆ›å»ºdataframe
        df = pd.DataFrame({
            'timestamp': timestamp_list,
            'cmdb_id': cmdb_id_list,
            'span_id': span_id_list,
            'trace_id': trace_id_list,
            'duration': duration_list,
            'type': type_list,
            'status_code': status_code_list,
            'operation_name': operation_name_list,
            'parent_span': parent_span_id
        })

        return df

    def trace_extract(self, start_time, end_time, path):
        print(start_time)
        original_start_time = start_time
        time_interval = 5 * 60
        csv_list = []
        os.makedirs(path, exist_ok=True)
        while start_time < end_time:
            current_end_time = start_time + time_interval
            if current_end_time > end_time:
                current_end_time = end_time
            data = self.trace_extract_(start_time=start_time, end_time=current_end_time)
            if len(data) != 0:
                data.to_csv(f'{path}/trace-{start_time}_{current_end_time}.csv', index=False)
                csv_list.append(f'{path}/trace-{start_time}_{current_end_time}.csv')
            start_time = current_end_time
            time.sleep(1)
            
        data_list = []
        for i, folder_path in enumerate(csv_list):
            data = pd.read_csv(folder_path)
            data_list.append(data)
            os.remove(folder_path)
        all_data = pd.concat(data_list)
        all_data = all_data.reset_index(drop=True)
        start_datetime = datetime.fromtimestamp(original_start_time)
        formatted_start_time = start_datetime.strftime("%Y-%m-%d_%H-%M-%S")
        file_path = os.path.join(path, f"trace_{formatted_start_time}.csv")
        all_data.to_csv(file_path)

    # traceæ•°æ®å¯¼å‡º
    def trace_extract_(self, start_time=None, end_time=None):
        trace_query_size = 5000

        if isinstance(start_time, int):
            start_time = datetime.fromtimestamp(start_time)
        if isinstance(end_time, int):
            end_time = datetime.fromtimestamp(end_time)

        start_time = self.timezone_adjust(start_time)
        end_time = self.timezone_adjust(end_time)

        query = {
            "size": trace_query_size,
            "query": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "@timestamp": {
                                    "gte": start_time,
                                    "lte": end_time
                                }
                            }
                        }
                    ]
                }
            },
            "sort": ["_doc"]
        }
        data = []

        st_time = time.time()
        try:
            page = self.elastic.search(index=self.indices_template, body=query, scroll='15s')

            data.extend(page["hits"]["hits"])
            scroll_id = page['_scroll_id']

            while True:
                page = self.elastic.scroll(scroll_id=scroll_id, scroll='15s')
                hits_len = len(page["hits"]["hits"])
                data.extend(page["hits"]["hits"])
                if hits_len < trace_query_size:
                    break
                scroll_id = page["_scroll_id"]
                time.sleep(1)

        except ConnectionTimeout as e:
            print('Connection Timeout:', e)

        df = self.trace_processing(data)
        ed_time = time.time()
        print('run time: ', ed_time - st_time)
        return df
    

class TraceMaintainer:
    def __init__(
        self,
        extractor: TraceExtractor,
        output_root: str = "./multi_modal_data",
        state_file: str = "trace_state.json",
        max_retries: int = 1
    ):
        self.extractor = extractor
        self.output_root = Path(output_root)
        self.state_file = Path(state_file)
        self.max_retries = max_retries
        self.state = self._load_state()
        self.running = False

    def _load_state(self) -> dict:
        if self.state_file.exists():
            try:
                with open(self.state_file, "r") as f:
                    return json.load(f)
            except Exception:
                return {}
        return {}

    def _save_state(self):
        try:
            with open(self.state_file, "w") as f:
                json.dump(self.state, f)
        except Exception as e:
            print(f"âš ï¸ Failed to save state: {e}")

    async def start(self):
        """å¯åŠ¨æ¯å°æ—¶å®šæ—¶ä»»åŠ¡"""
        self.running = True
        while self.running:
            now = datetime.now(timezone.utc)
            next_hour = (now + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)
            wait_seconds = (next_hour - now).total_seconds()
            await asyncio.sleep(wait_seconds)
            await self._extract_previous_hour()

    async def stop(self):
        self.running = False

    async def _extract_previous_hour(self):
        end_dt = datetime.now(timezone.utc).replace(minute=0, second=0, microsecond=0)
        start_dt = end_dt - timedelta(hours=1)
        await self.extract_for_hour(start_dt)

    async def extract_for_hour(self, start_dt_utc: datetime):
        if start_dt_utc.tzinfo != timezone.utc:
            raise ValueError("start_dt_utc å¿…é¡»æ˜¯ UTC æ—¶é—´")

        hour_key = start_dt_utc.strftime("%Y-%m-%d_%H:%M:%S")
        if self.state.get(hour_key, False):
            print(f"â© {hour_key} å·²æå–ï¼Œè·³è¿‡")
            return

        end_dt_utc = start_dt_utc + timedelta(hours=1)
        print(f"ğŸ“¦ æå–Trace: {start_dt_utc} ~ {end_dt_utc} (UTC)")

        tz_utc8 = ZoneInfo("Asia/Shanghai")
        date_str = start_dt_utc.astimezone(tz_utc8).strftime("%Y-%m-%d")
        output_dir = self.output_root / date_str / "trace"
        output_dir.mkdir(parents=True, exist_ok=True)

        success = False
        for attempt in range(1, self.max_retries + 1):
            try:
                await asyncio.to_thread(
                    self.extractor.trace_extract,
                    int(start_dt_utc.timestamp()),
                    int(end_dt_utc.timestamp()),
                    output_dir
                )
                success = True
                break
            except Exception as e:
                wait_time = 2 ** attempt
                print(f"âŒ ç¬¬{attempt}æ¬¡æå–å¤±è´¥: {e}ï¼Œ{wait_time}såé‡è¯•...")
                await asyncio.sleep(wait_time)

        if success:
            self.state[hour_key] = True
            self._save_state()


# ========================
# Shortcut å·¥å…·å‡½æ•°
# ========================
async def extract_full_day(
    maintainer: TraceMaintainer,
    date_str: str,
    tz: timezone = timezone.utc
):
    """æå–æŒ‡å®šæ—¥æœŸçš„å®Œæ•´ traceï¼ˆæŒ‰å°æ—¶ï¼‰"""
    start_dt_local = datetime.strptime(date_str, "%Y-%m-%d").replace(tzinfo=tz)
    start_dt_utc = start_dt_local.astimezone(timezone.utc)

    for h in range(6):
        hour_dt = start_dt_utc + timedelta(hours=h)
        await maintainer.extract_for_hour(hour_dt)


# ========================
# ä½¿ç”¨ç¤ºä¾‹
# ========================
async def main():
    extractor = TraceExtractor(
        url="https://localhost:9200",
        username="elastic",
        password="elastic"
    )
    
    maintainer = TraceMaintainer(extractor, output_root="./multi_modal_data")

    # å¯åŠ¨å®šæ—¶æ¨¡å¼
    # await maintainer.start()

    # æˆ–è€…ï¼šæå–æŸä¸€å¤©çš„æ‰€æœ‰ trace
    await extract_full_day(maintainer, "2025-08-19", tz=timezone(timedelta(hours=8)))  # è¾“å…¥åŒ—äº¬æ—¶é—´


if __name__ == "__main__":
    asyncio.run(main())
